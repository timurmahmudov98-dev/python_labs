import pytest
from lib_4text.text import normalize, tokenize, count_freq, top_n


@pytest.mark.parametrize(  # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    "source, expected",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
        ("Hello\r\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),  # –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        ("  ", ""),  # –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
    ],
)
def test_normalize(source, expected):
    assert normalize(source) == expected  # –ø—Ä–æ–≤–µ—Ä–∫–∞


@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("hello,world!!!", ["hello", "world"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
        ("", []),  # –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        ("!@#$%^&*()", []),  # —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        ("  ", []),  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
    ],
)
def test_tokenize(source, expected):
    assert tokenize(source) == expected


@pytest.mark.parametrize(
    "source, expected",
    [
        (["a", "b", "a", "c", "b", "a"], {"a": 3, "b": 2, "c": 1}),
        (["bb", "aa", "bb", "aa", "cc"], {"aa": 2, "bb": 2, "cc": 1}),
        ([], {}),  # –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
    ],
)
def test_count_freq(source, expected):
    assert count_freq(source) == expected


@pytest.mark.parametrize(
    "source, n, expected",
    [
        ({"a": 3, "b": 2, "c": 1}, 2, [("a", 3), ("b", 2)]),
        ({"aa": 2, "bb": 2, "cc": 1}, 2, [("aa", 2), ("bb", 2)]),
        ({}, 5, []),  # –ø—É—Å—Ç—ã–µ —Å–ª–æ–≤–∞—Ä–∏
    ],
)
def test_top_n(source, n, expected):
    assert top_n(source, n) == expected
